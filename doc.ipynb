{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fb8a47c",
   "metadata": {},
   "source": [
    "### <center>QuakeMatch </center>\n",
    "### <center>Luca Longo, Università degli Studi di Catania </center>\n",
    "<img src=\"https://user-images.githubusercontent.com/50534107/256270970-f0de1c54-ba7f-46a0-ab32-c4ad00e6b26e.svg\" alt=\"QuakeMatch\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fc9b2a",
   "metadata": {},
   "source": [
    "## Table of contents:\n",
    "* [Introduction](#Introduction)\n",
    "* [Quake Match Pipeline](#QuakeMatchPipeline)\n",
    "* [Data Ingestion](#dataIngestion)\n",
    "    * [Python Script](#pythonScript)\n",
    "    * [Logstash](#logstash)\n",
    "* [Streaming with Kafka](#kafka)\n",
    "* [Processing with Apache Spark](#spark)\n",
    "* [Data indexing with Elasticsearch](#elasticsearch)\n",
    "* [Data visualization with Kibana](#kibana)\n",
    "* [Requirements](#requirements)\n",
    "* [Usage](#usage)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "360c3c59",
   "metadata": {},
   "source": [
    "# Introduction <a class=\"anchor\" id=\"Introduction\"></a>\n",
    "\n",
    "<p>\n",
    "QuakeMatch is a tool for matching detection of antipodal earthquakes that use sismics data to look for a match between two or more events. It uses technologies such as Logstash, Kafka with Kafka Zookeeper, Apache Spark, Elasticsearch and Kibana for data ingestion, filtering, elaboration and results visualization.\n",
    "</p>\n",
    "\n",
    "## Quake Match Pipeline<a class=\"anchor\" id=\"QuakeMatchPipeline\"></a>\n",
    "<p>\n",
    "    <img src=\"https://user-images.githubusercontent.com/50534107/256268865-21fcd7c9-1762-45da-8a8e-aed7e15b7468.png\" alt=\"Pipeline\" />\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39814127",
   "metadata": {},
   "source": [
    "# Data Ingestion <a class=\"anchor\" id=\"dataIngestion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0566e125-8383-4253-b1de-4aeaa54de263",
   "metadata": {},
   "source": [
    "## Python Script <a class=\"anchor\" id=\"pythonScript\"></a>\n",
    "The following code allows you to generate a file containing all the urls needed for the API requests made by Logstash:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d65b23ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "base_url = \"https://www.seismicportal.eu/fdsnws/event/1/query?limit=7000&start={}&end={}\"\n",
    "\n",
    "start_date = datetime.datetime(1998, 7, 19)\n",
    "end_date = datetime.datetime(2023, 7, 19)\n",
    "\n",
    "current_date = start_date\n",
    "links = []\n",
    "\n",
    "i=0\n",
    "while current_date <= end_date:\n",
    "    start_time = current_date.strftime(\"%Y-%m-%dT%H:%M:%S.0\")\n",
    "    end_time = (current_date + datetime.timedelta(days=1) - datetime.timedelta(seconds=1)).strftime(\"%Y-%m-%dT%H:%M:%S.0\")\n",
    "    link = base_url.format(start_time, end_time)\n",
    "    links.append(link)\n",
    "    current_date += datetime.timedelta(days=1)\n",
    "    i=i+1\n",
    "\n",
    "with open(\"./logstash/seismic_portal_links.txt\", \"w\") as file:\n",
    "    for link in links:\n",
    "        file.write(link + \"\\n\")\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3041328e",
   "metadata": {},
   "source": [
    "You can choose the date range by modifying the start_date and end_date values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d61b02-4b74-4170-a902-a4c96e675265",
   "metadata": {},
   "source": [
    "## Logstash <a class=\"anchor\" id=\"logstash\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d695379",
   "metadata": {},
   "source": [
    "Logstash will read the file generated by the python script, execute the API requests to Seismic Portal, filter the data and send the generated messages to Kafka."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad12a7b",
   "metadata": {},
   "source": [
    "# Streaming with Kafka <a class=\"anchor\" id=\"kafka\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d3fbfb",
   "metadata": {},
   "source": [
    "Messages sent from Logstash to Kafka will be in the \"earthquakes\" topic. Each message will present five lists including all timestamps, regions, latitudes, longitudes and magnitudes of all events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5f3920",
   "metadata": {},
   "source": [
    "# Processing with Apache Spark <a class=\"anchor\" id=\"spark\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c952b91",
   "metadata": {},
   "source": [
    "Spark will take care of the following tasks:\n",
    "<ul>\n",
    "    <li>Taking messages from the Kafka topic \"earthquakes\"</li>\n",
    "    <li>Filtering by magnitude ≥ 5.5</li>\n",
    "    <li>Antipode calculation</li>\n",
    "    <li>Retrieving a match with one or more events whose latitude and longitude is in the range of ± 30° with respect to the calculated antipode and which occurred within the following three days</li>\n",
    "    <li>Creating a CSV file with the obtained data</li>\n",
    "    <li>Creating an index on Elasticsearch</li>\n",
    "    <li>Data indexing</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54966044",
   "metadata": {},
   "source": [
    "# Data indexing with Elasticsearch <a class=\"anchor\" id=\"elasticsearch\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0580de8e",
   "metadata": {},
   "source": [
    "The data will be indexed in Elasticsearch under the index \"earthquakes\" and mapping \"earthquake_mapping\". Each item will have the following fields:\n",
    "<ul>\n",
    "    <li>unique id</li>\n",
    "    <li>timestamp</li>\n",
    "    <li>region</li>\n",
    "    <li>latitude</li>\n",
    "    <li>longitude</li>\n",
    "    <li>latitude_antipode</li>\n",
    "    <li>longitude_antipode</li>\n",
    "    <li>matches</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7eca74",
   "metadata": {},
   "source": [
    "# Data visualization with Kibana <a class=\"anchor\" id=\"kibana\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebaf6ca",
   "metadata": {},
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/50534107/256273468-eec68964-ff00-4630-9f45-3998e94b6037.png\" alt=\"kibana\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3682e6f",
   "metadata": {},
   "source": [
    "The Kibana dashboard will show five lenses: the first indicates the number of matches that QuakeMatch was able to detect, followed by a descriptive table of the matches and finally three other lenses that show the data of the earthquakes occurred in Italy, the distribution of the number of earthquakes by magnitude and the distribution of earthquakes in the european area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2295430a",
   "metadata": {},
   "source": [
    "# Requirements <a class=\"anchor\" id=\"requirements\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b05d42c",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Docker</li>\n",
    "    <li>Python</li>\n",
    "    <li>wget</li>\n",
    "    <li>A solution with, at least, 16GB of RAM</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1087895",
   "metadata": {},
   "source": [
    "# Usage <a class=\"anchor\" id=\"usage\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f8cbb0-af8c-4f90-93e2-fd58f34978cb",
   "metadata": {},
   "source": [
    "<li>Install Docker in your system, than run the following command for create a docker network:</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d39ea3dd-0d75-49f9-92ae-f81830bc3eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error response from daemon: network with name kafka-network already exists\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'. ~/.bashrc\\ndocker network create kafka-network\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m. ~/.bashrc\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdocker network create kafka-network\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:2478\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2476\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2477\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2478\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2481\u001b[0m \u001b[38;5;66;03m# when using magics with decodator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2482\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/magics/script.py:153\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/magics/script.py:305\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'. ~/.bashrc\\ndocker network create kafka-network\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    ". ~/.bashrc\n",
    "docker network create kafka-network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea528b5-6e54-44fb-bba6-a1aa40e08f29",
   "metadata": {},
   "source": [
    "<li>Create a container with Kafka Zookeeper:</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0cc9e9c-efac-4455-a5a9-603bf0fd4e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20954fd1d3ac17271ee7af18712431b76fdfd715ead1ae43801c24435ac4164a\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    ". ~/.bashrc\n",
    "docker run -d \\\n",
    "  --name zookeeper \\\n",
    "  --network kafka-network \\\n",
    "  -p 2181:2181 \\\n",
    "  -e ZOOKEEPER_CLIENT_PORT=2181 \\\n",
    "  confluentinc/cp-zookeeper:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fa14bb-68f2-4b09-ab89-54ca167d96c7",
   "metadata": {},
   "source": [
    "<li>Wait Zookeper to be fully loaded, then create a container with Kafka:</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dbbb62e-74bb-4e6e-a744-39677b39ed0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84adbd04597e07a877e2d919e24b8ac86f564ad0d1725ac273bd0d811b94dc7a\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    ". ~/.bashrc\n",
    "docker run -d \\\n",
    "  --name kafka \\\n",
    "  --network kafka-network \\\n",
    "  -p 9092:9092 \\\n",
    "  -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \\\n",
    "  -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \\\n",
    "  -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \\\n",
    "  confluentinc/cp-kafka:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bf63ac-c5f3-47a5-8943-951b568cebde",
   "metadata": {},
   "source": [
    "<li>And a container with Kafka UI:</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea33a13c-81aa-463d-8a18-3db76c7abf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e7203654af8b480a34e790900fa902152c495d3569c693fcb1a35a19da140624\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    ". ~/.bashrc\n",
    "docker run -d \\\n",
    "  --name kafka-ui \\\n",
    "  --network kafka-network \\\n",
    "  -p 8080:8080 \\\n",
    "  -e KAFKA_CLUSTERS_0_NAME=local \\\n",
    "  -e KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:9092 \\\n",
    "  -e KAFKA_CLUSTERS_0_ZOOKEEPER=zookeeper:2181 \\\n",
    "  -e KAFKA_CLUSTERS_0_ENABLESR=false \\\n",
    "  -e KAFKA_CLUSTERS_0_SASLMECHANISM= \\\n",
    "  -e KAFKA_CLUSTERS_0_SASLPLAIN_USERNAME= \\\n",
    "  -e KAFKA_CLUSTERS_0_SASLPLAIN_PASSWORD= \\\n",
    "  -e KAFKA_CLUSTERS_0_SASLPLAIN_PASSWORD_FILE= \\\n",
    "  -e KAFKA_CLUSTERS_0_TRUSTEDCERTS= \\\n",
    "  -e KAFKA_CLUSTERS_0_CLIENTCERT= \\\n",
    "  -e KAFKA_CLUSTERS_0_CLIENTKEY= \\\n",
    "  -e KAFKA_CLUSTERS_0_CLIENTKEYPASSWORD= \\\n",
    "  -e KAFKA_CLUSTERS_0_CONSUMERCONFIGS= \\\n",
    "  -e KAFKA_CLUSTERS_0_ADMINCONFIGS= \\\n",
    "  provectuslabs/kafka-ui:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d8323c-96a3-424c-aa0e-6840626cbe25",
   "metadata": {},
   "source": [
    "<li>Create Elasticsearch docker image:</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfd185d4-69d7-4d84-9af1-4b9166e5e3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#0 building with \"default\" instance using docker driver\n",
      "\n",
      "#1 [internal] load .dockerignore\n",
      "#1 transferring context: 2B done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [internal] load build definition from Dockerfile\n",
      "#2 transferring dockerfile: 207B done\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#3 [internal] load metadata for docker.elastic.co/elasticsearch/elasticsearch:7.17.0\n",
      "#3 DONE 1.4s\n",
      "\n",
      "#4 [1/2] FROM docker.elastic.co/elasticsearch/elasticsearch:7.17.0@sha256:577b382dda5d05385aea8c7b60dad97e02ff41ca0da54f723151c2aed9ac8f54\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#5 [2/2] RUN sysctl -w vm.max_map_count=262144\n",
      "#5 CACHED\n",
      "\n",
      "#6 exporting to image\n",
      "#6 exporting layers done\n",
      "#6 writing image sha256:1191ba2020cb7d705cf82c256ef21733d9c8ec31ae0fc109e4c9619ad716da12 done\n",
      "#6 naming to docker.io/library/elastic-image done\n",
      "#6 DONE 0.0s\n",
      "\n",
      "What's Next?\n",
      "  View summary of image vulnerabilities and recommendations → docker scout quickview\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    ". ~/.bashrc\n",
    "docker build -t elastic-image ./elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b4575b-4fd9-4b9c-9e5c-0a079c94e204",
   "metadata": {},
   "source": [
    "<li>Then run Elasticsearch:</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bfcc177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b83ec72383241a1896816a9a956787bb17fa553f379370a2a97e944c2e8ae5c1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    ". ~/.bashrc\n",
    "docker run -d --name elasticsearch --network kafka-network -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" elastic-image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2a139c-37f3-4398-b498-67c0529af2a8",
   "metadata": {},
   "source": [
    "<li>Create a Kibana container:</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28bffbdc-61e6-4b97-99de-980fc83987c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46ae3311054b64b58fbb1ffbc745f601177570ccbdcf1ff8409c427a21d150c2\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    ". ~/.bashrc\n",
    "docker run -d --name kibana -p 5601:5601 --network kafka-network -e \"ELASTICSEARCH_HOSTS=http://elasticsearch:9200\" docker.elastic.co/kibana/kibana:7.15.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9857ba-39f4-4c98-9a33-1fbd35a4ea33",
   "metadata": {},
   "source": [
    "<li>Run python script:</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e8f7c86-bdb9-439e-a3e6-a3ca27f9065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    ". ~/.bashrc\n",
    "python3 ./logstash/urls_dates.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8c4d72-713b-4bc4-ba6c-83bf1b572eae",
   "metadata": {},
   "source": [
    "<li>Create Logstash docker image:</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "958290c0-50ed-4c19-b88e-e146c5fc0582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#0 building with \"default\" instance using docker driver\n",
      "\n",
      "#1 [internal] load build definition from Dockerfile\n",
      "#1 transferring dockerfile: 503B done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [internal] load .dockerignore\n",
      "#2 transferring context: 2B done\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#3 [internal] load metadata for docker.elastic.co/logstash/logstash:8.8.1@sha256:9b2e080605e208ef1165fd6cfd68a8b05c2031c8818b8520f82f73238dbb471c\n",
      "#3 DONE 0.0s\n",
      "\n",
      "#4 [1/6] FROM docker.elastic.co/logstash/logstash:8.8.1@sha256:9b2e080605e208ef1165fd6cfd68a8b05c2031c8818b8520f82f73238dbb471c\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#5 [internal] load build context\n",
      "#5 transferring context: 80B done\n",
      "#5 DONE 0.0s\n",
      "\n",
      "#6 [2/6] COPY logstash.conf /usr/share/logstash/pipeline/logstash.conf\n",
      "#6 CACHED\n",
      "\n",
      "#7 [3/6] COPY seismic_portal_links.txt /usr/share/logstash/seismic_portal_links.txt\n",
      "#7 CACHED\n",
      "\n",
      "#8 [4/6] RUN logstash-plugin install logstash-filter-xml\n",
      "#8 CACHED\n",
      "\n",
      "#9 [5/6] RUN logstash-plugin install logstash-filter-mutate\n",
      "#9 CACHED\n",
      "\n",
      "#10 [6/6] RUN logstash-plugin install logstash-input-http\n",
      "#10 CACHED\n",
      "\n",
      "#11 exporting to image\n",
      "#11 exporting layers done\n",
      "#11 writing image sha256:e7dea43726f33d47465f6c6a0d645930dbd82ee86f039502b4079d28ef6f313f done\n",
      "#11 naming to docker.io/library/logstash-image done\n",
      "#11 DONE 0.0s\n",
      "\n",
      "What's Next?\n",
      "  View summary of image vulnerabilities and recommendations → docker scout quickview\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    ". ~/.bashrc\n",
    "docker build -t logstash-image ./logstash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e59e0ef-39d9-4f28-a90b-4ea78df6be9b",
   "metadata": {},
   "source": [
    "<li>Then run Logstash:</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35060c65-e8ff-4adf-bc30-7759bb322e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8c039f5496a6b1b143e0a443b0de68193587ad26e85042733000dcaa072c768c\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    ". ~/.bashrc\n",
    "docker run -d --name logstash-container --network kafka-network  logstash-image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baa7ec7-b449-4a7f-937f-ec7eb8e956dd",
   "metadata": {},
   "source": [
    "<li>Run following code for download elasticsearch-spark</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41b69312-1947-490d-9c31-9963bcab4ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-07-27 19:24:45--  https://repo1.maven.org/maven2/org/elasticsearch/elasticsearch-spark-20_2.12/7.15.1/elasticsearch-spark-20_2.12-7.15.1.jar\n",
      "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
      "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2101583 (2.0M) [application/java-archive]\n",
      "Saving to: ‘./spark/elasticsearch-spark-20_2.12-7.15.1.jar’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  2% 3.50M 1s\n",
      "    50K .......... .......... .......... .......... ..........  4% 4.24M 0s\n",
      "   100K .......... .......... .......... .......... ..........  7% 17.7M 0s\n",
      "   150K .......... .......... .......... .......... ..........  9% 20.0M 0s\n",
      "   200K .......... .......... .......... .......... .......... 12% 6.69M 0s\n",
      "   250K .......... .......... .......... .......... .......... 14% 42.9M 0s\n",
      "   300K .......... .......... .......... .......... .......... 17% 33.5M 0s\n",
      "   350K .......... .......... .......... .......... .......... 19% 28.8M 0s\n",
      "   400K .......... .......... .......... .......... .......... 21% 8.19M 0s\n",
      "   450K .......... .......... .......... .......... .......... 24% 86.4M 0s\n",
      "   500K .......... .......... .......... .......... .......... 26% 28.4M 0s\n",
      "   550K .......... .......... .......... .......... .......... 29% 53.9M 0s\n",
      "   600K .......... .......... .......... .......... .......... 31% 48.6M 0s\n",
      "   650K .......... .......... .......... .......... .......... 34% 38.2M 0s\n",
      "   700K .......... .......... .......... .......... .......... 36% 93.8M 0s\n",
      "   750K .......... .......... .......... .......... .......... 38%  143M 0s\n",
      "   800K .......... .......... .......... .......... .......... 41% 73.6M 0s\n",
      "   850K .......... .......... .......... .......... .......... 43% 9.32M 0s\n",
      "   900K .......... .......... .......... .......... .......... 46%  159M 0s\n",
      "   950K .......... .......... .......... .......... .......... 48%  122M 0s\n",
      "  1000K .......... .......... .......... .......... .......... 51% 34.7M 0s\n",
      "  1050K .......... .......... .......... .......... .......... 53% 89.2M 0s\n",
      "  1100K .......... .......... .......... .......... .......... 56%  137M 0s\n",
      "  1150K .......... .......... .......... .......... .......... 58%  117M 0s\n",
      "  1200K .......... .......... .......... .......... .......... 60%  111M 0s\n",
      "  1250K .......... .......... .......... .......... .......... 63%  115M 0s\n",
      "  1300K .......... .......... .......... .......... .......... 65% 72.6M 0s\n",
      "  1350K .......... .......... .......... .......... .......... 68% 74.7M 0s\n",
      "  1400K .......... .......... .......... .......... .......... 70%  116M 0s\n",
      "  1450K .......... .......... .......... .......... .......... 73%  103M 0s\n",
      "  1500K .......... .......... .......... .......... .......... 75% 93.0M 0s\n",
      "  1550K .......... .......... .......... .......... .......... 77%  116M 0s\n",
      "  1600K .......... .......... .......... .......... .......... 80%  113M 0s\n",
      "  1650K .......... .......... .......... .......... .......... 82%  131M 0s\n",
      "  1700K .......... .......... .......... .......... .......... 85%  113M 0s\n",
      "  1750K .......... .......... .......... .......... .......... 87% 12.4M 0s\n",
      "  1800K .......... .......... .......... .......... .......... 90%  126M 0s\n",
      "  1850K .......... .......... .......... .......... .......... 92%  105M 0s\n",
      "  1900K .......... .......... .......... .......... .......... 95%  120M 0s\n",
      "  1950K .......... .......... .......... .......... .......... 97%  131M 0s\n",
      "  2000K .......... .......... .......... .......... .......... 99%  114M 0s\n",
      "  2050K ..                                                    100% 4439G=0.08s\n",
      "\n",
      "2023-07-27 19:24:45 (26.7 MB/s) - ‘./spark/elasticsearch-spark-20_2.12-7.15.1.jar’ saved [2101583/2101583]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    ". ~/.bashrc\n",
    "wget https://repo1.maven.org/maven2/org/elasticsearch/elasticsearch-spark-20_2.12/7.15.1/elasticsearch-spark-20_2.12-7.15.1.jar -P ./spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca2b6f4-29e2-4710-8479-38f0451301bf",
   "metadata": {},
   "source": [
    "<li>And spark-sql-kafka:</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d5bf531-9c5b-4915-a8c4-38ca6c412cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-07-27 19:24:54--  https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.4.1/spark-sql-kafka-0-10_2.12-3.4.1.jar\n",
      "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
      "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 427253 (417K) [application/java-archive]\n",
      "Saving to: ‘./spark/spark-sql-kafka-0-10_2.12-3.4.1.jar’\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 11% 3.47M 0s\n",
      "    50K .......... .......... .......... .......... .......... 23% 4.47M 0s\n",
      "   100K .......... .......... .......... .......... .......... 35% 18.8M 0s\n",
      "   150K .......... .......... .......... .......... .......... 47% 18.9M 0s\n",
      "   200K .......... .......... .......... .......... .......... 59% 6.95M 0s\n",
      "   250K .......... .......... .......... .......... .......... 71% 37.7M 0s\n",
      "   300K .......... .......... .......... .......... .......... 83% 25.2M 0s\n",
      "   350K .......... .......... .......... .......... .......... 95% 35.4M 0s\n",
      "   400K .......... .......                                    100% 23.1M=0.04s\n",
      "\n",
      "2023-07-27 19:24:54 (9.58 MB/s) - ‘./spark/spark-sql-kafka-0-10_2.12-3.4.1.jar’ saved [427253/427253]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    ". ~/.bashrc\n",
    "wget https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.4.1/spark-sql-kafka-0-10_2.12-3.4.1.jar -P ./spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833f7de0-70a6-4be7-9554-53452beb1b1b",
   "metadata": {},
   "source": [
    "<li>Create Apache Spark docker image:</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78daf47c-d945-4b16-9d4d-5714fc8c0872",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#0 building with \"default\" instance using docker driver\n",
      "\n",
      "#1 [internal] load build definition from Dockerfile\n",
      "#1 transferring dockerfile: 281B done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [internal] load .dockerignore\n",
      "#2 transferring context: 2B done\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#3 [internal] load metadata for docker.io/bitnami/spark:latest\n",
      "#3 DONE 1.7s\n",
      "\n",
      "#4 [internal] load build context\n",
      "#4 transferring context: 139B done\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#5 [1/6] FROM docker.io/bitnami/spark:latest@sha256:9467c6ec2cfd0cde0cb23ea81f44f85430cb0a8154d8a06982ec8895b1734b00\n",
      "#5 resolve docker.io/bitnami/spark:latest@sha256:9467c6ec2cfd0cde0cb23ea81f44f85430cb0a8154d8a06982ec8895b1734b00 0.0s done\n",
      "#5 sha256:9467c6ec2cfd0cde0cb23ea81f44f85430cb0a8154d8a06982ec8895b1734b00 529B / 529B done\n",
      "#5 sha256:219f5d3a1e16c649755fafa7b036617e9512bfb743d3a928cf6b6d0dc67698fc 430B / 430B done\n",
      "#5 sha256:31fc611bafaf2f5ca78e64a1558568050417c31d036aa2f7166f7eb02c2a014e 7.92kB / 7.92kB done\n",
      "#5 sha256:8ef45242f2ce875baf17b450ce8518e0993b113a0b3ddda37ad2170d85caafbe 0B / 984.72MB 0.1s\n",
      "#5 sha256:8ef45242f2ce875baf17b450ce8518e0993b113a0b3ddda37ad2170d85caafbe 56.62MB / 984.72MB 1.5s\n",
      "#5 sha256:8ef45242f2ce875baf17b450ce8518e0993b113a0b3ddda37ad2170d85caafbe 109.05MB / 984.72MB 2.5s\n",
      "#5 sha256:8ef45242f2ce875baf17b450ce8518e0993b113a0b3ddda37ad2170d85caafbe 160.43MB / 984.72MB 3.6s\n",
      "#5 sha256:8ef45242f2ce875baf17b450ce8518e0993b113a0b3ddda37ad2170d85caafbe 209.72MB / 984.72MB 4.6s\n",
      "#5 sha256:8ef45242f2ce875baf17b450ce8518e0993b113a0b3ddda37ad2170d85caafbe 265.29MB / 984.72MB 6.0s\n",
      "#5 sha256:8ef45242f2ce875baf17b450ce8518e0993b113a0b3ddda37ad2170d85caafbe 326.11MB / 984.72MB 7.2s\n",
      "#5 sha256:8ef45242f2ce875baf17b450ce8518e0993b113a0b3ddda37ad2170d85caafbe 377.49MB / 984.72MB 8.3s\n",
      "#5 sha256:8ef45242f2ce875baf17b450ce8518e0993b113a0b3ddda37ad2170d85caafbe 439.35MB / 984.72MB 9.7s\n",
      "#5 sha256:8ef45242f2ce875baf17b450ce8518e0993b113a0b3ddda37ad2170d85caafbe 501.22MB / 984.72MB 11.1s\n",
      "#5 sha256:8ef45242f2ce875baf17b450ce8518e0993b113a0b3ddda37ad2170d85caafbe 553.65MB / 984.72MB 12.2s\n",
      "#5 sha256:8ef45242f2ce875baf17b450ce8518e0993b113a0b3ddda37ad2170d85caafbe 610.27MB / 984.72MB 13.3s\n",
      "#5 sha256:8ef45242f2ce875baf17b450ce8518e0993b113a0b3ddda37ad2170d85caafbe 659.55MB / 984.72MB 14.4s\n",
      "#5 sha256:8ef45242f2ce875baf17b450ce8518e0993b113a0b3ddda37ad2170d85caafbe 719.32MB / 984.72MB 15.4s\n",
      "#5 sha256:8ef45242f2ce875baf17b450ce8518e0993b113a0b3ddda37ad2170d85caafbe 770.70MB / 984.72MB 16.4s\n",
      "#5 sha256:8ef45242f2ce875baf17b450ce8518e0993b113a0b3ddda37ad2170d85caafbe 830.47MB / 984.72MB 17.1s\n",
      "#5 sha256:8ef45242f2ce875baf17b450ce8518e0993b113a0b3ddda37ad2170d85caafbe 896.53MB / 984.72MB 17.7s\n",
      "#5 sha256:8ef45242f2ce875baf17b450ce8518e0993b113a0b3ddda37ad2170d85caafbe 958.40MB / 984.72MB 18.3s\n",
      "#5 extracting sha256:8ef45242f2ce875baf17b450ce8518e0993b113a0b3ddda37ad2170d85caafbe\n",
      "#5 sha256:8ef45242f2ce875baf17b450ce8518e0993b113a0b3ddda37ad2170d85caafbe 984.72MB / 984.72MB 18.6s done\n",
      "#5 extracting sha256:8ef45242f2ce875baf17b450ce8518e0993b113a0b3ddda37ad2170d85caafbe 5.1s\n",
      "#5 extracting sha256:8ef45242f2ce875baf17b450ce8518e0993b113a0b3ddda37ad2170d85caafbe 10.1s\n",
      "#5 extracting sha256:8ef45242f2ce875baf17b450ce8518e0993b113a0b3ddda37ad2170d85caafbe 13.0s done\n",
      "#5 DONE 31.9s\n",
      "\n",
      "#6 [2/6] WORKDIR /app\n",
      "#6 DONE 1.0s\n",
      "\n",
      "#7 [3/6] COPY earthquake_analysis.py .\n",
      "#7 DONE 0.1s\n",
      "\n",
      "#8 [4/6] COPY requirements.txt .\n",
      "#8 DONE 0.1s\n",
      "\n",
      "#9 [5/6] COPY elasticsearch-spark-20_2.12-7.15.1.jar /opt/bitnami/spark/jars/\n",
      "#9 DONE 0.1s\n",
      "\n",
      "#10 [6/6] RUN pip install -r requirements.txt\n",
      "#10 0.598 WARNING: The directory '/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\n",
      "#10 0.600 Defaulting to user installation because normal site-packages is not writeable\n",
      "#10 0.850 Collecting py4j==0.10.9\n",
      "#10 0.934   Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
      "#10 1.010 Collecting kafka-python==2.0.2\n",
      "#10 1.026   Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "#10 1.268 Collecting elasticsearch==7.14.0\n",
      "#10 1.288   Downloading elasticsearch-7.14.0-py2.py3-none-any.whl (364 kB)\n",
      "#10 1.389 Collecting urllib3<2,>=1.21.1\n",
      "#10 1.404   Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "#10 1.454 Collecting certifi\n",
      "#10 1.477   Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "#10 1.576 Installing collected packages: urllib3, certifi, py4j, kafka-python, elasticsearch\n",
      "#10 1.932 ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "#10 1.932 pyspark 3.4.1 requires py4j==0.10.9.7, but you have py4j 0.10.9 which is incompatible.\n",
      "#10 1.932 Successfully installed certifi-2023.7.22 elasticsearch-7.14.0 kafka-python-2.0.2 py4j-0.10.9 urllib3-1.26.16\n",
      "#10 DONE 2.2s\n",
      "\n",
      "#11 exporting to image\n",
      "#11 exporting layers 0.1s done\n",
      "#11 writing image sha256:c1c349eeae17558440411f702abb77536eb9cc1546e8dcd16cc5f9ecbe6cf36b done\n",
      "#11 naming to docker.io/library/spark-earthquakes done\n",
      "#11 DONE 0.2s\n",
      "\n",
      "What's Next?\n",
      "  View summary of image vulnerabilities and recommendations → docker scout quickview\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    ". ~/.bashrc\n",
    "docker build -t spark-earthquakes ./spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decada74-cf98-4356-8ba0-cbb3514c0a71",
   "metadata": {},
   "source": [
    "<li>Wait until all messages in Kafka's \"earthquakes\" topic are ready, then run Apache Spark container:</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ed0ce5-2088-4ca1-9efa-b50176450d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    ". ~/.bashrc\n",
    "docker run -d --network kafka-network --name spark-earthquakes_analyzer spark-earthquakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e925e9-95f5-4c04-9309-d9458b93b5b6",
   "metadata": {},
   "source": [
    "<li>In the browser, put the url \"http://localhost:5601\", go to \"Kibana / Saved Objects\", click on \"import\" and select \"export.ndjson\" in kibana folder</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb02840-18dd-4137-bbaf-8a4984ba1a82",
   "metadata": {},
   "source": [
    "<li>Open left menu in Kibana, select Dashboard, select the imported dashboard for see all lens</li>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
